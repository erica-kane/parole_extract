# Turning text into data:<br><small>_Automatically extracting information from Parole Board decision letters._</small>

This repository contains the code developed during my PhD research, which focuses on extracting decision-relevant information from Parole Board final decision letters. These letters are unstructured Word documents, written by Parole Board members after making decisions on a prisoner's progression. These decisions arise from two processes: Member Case Assessments (MCA) and Oral Hearings (OH), each generating distinct decision letters. 

Currently, the Parole Board stores only limited administrative and demographic data in an analysable Excel files, leaving key decision-making information locked within the letters. This project aims to process these unstructured documents to extract and add relevant decision-making data to the existing administrative records. This will enable comprehensive, large-scale statistical analyses of Parole Board decision-making.

## Repository structure

### `data/`
- **original_data**: Synthetic Parole Board documents.
- **primary_data**: Synthetic Parole Board decision letters in various formats.
- **supplementary_data**: Synthetic Parole Board administrative data.
- **linked_data**: Files combining primary and supplementary data after linkage.
- **models**: The pre-trained models used throughout the processing stages. 
 
### `scripts/`

- **docloader.py**: Reads and caches text documents from a specified folder.
- **dl_classification.R**: Reads, classifies, and sorts files as decision letters.
- **pseudonymisation.ipynb**: Removes direct personal identifiers from decision letters.
- **segmentation.ipynb**: Reduces decision letters to relevant sections.
- **extraction.ipynb**: Extracts convicted crime entities from decision letters.
- **simplification.ipynb**: Classifies and reformats extracted crime entities.
- **linkage.ipynb**: Links unique identifiers and crime entities to supplementary data
- **preparation.ipynb**: Refines linked data by adding, reformatting, and removing variables.

## Requirements

To run the scripts in this repository, you will need a series of R and Python libraries.  

üìä R libraries:  
- `dplyr`
- `purrr`
- `readtext`
- `stringr`
- `tibble`

üêç Python libraries:
- `joblib`
- `numpy`
- `pandas`
- `sklearn`
- `spacy`
- `textract`
  
You will also need to install the `en_core_web_trf` model from `spaCy`. To install the model, run the following command:

```bash
python -m spacy download en_core_web_trf
```

The project also requires `antiword`, a tool for extracting text from Microsoft Word `.doc` files (a common file type of Parole Board decision letters). This installation process will depend on your operating system.

## Usage

The scripts in this repository are intended to be executed sequentially, with each script building upon the outputs generated by the previous one. After each run, the output is saved in the relevant folder of the `data/` directory for inspection. This step-by-step process ensures transparency, which is crucial to the integrity of the project's methodology.

The pipeline follows this flow:

### ‚úâÔ∏è 1. Decision letter classification 

**Script**: `scripts/dl_classification.R`

**Input**: 
- `data/original_data/`: Parole Board files (.doc, .docx, .pdf). 

**Output**: 
- `data/primary_data/letters/original_dls/`: MCA and OH decision letters (.doc, .docx).

### üîí 2. Pseudonymisation 

**Script**: `scripts/pseudonymisation.ipynb`

**Input**: 
- `data/primary_data/letters/original_dls/`: MCA and OH decision letters (.doc, .docx).

**Output**: 
- `data/primary_data/letters/pseudon_dls/`: pseudonymised MCA and OH decision letters (.txt).

### ‚úÇÔ∏è 3. Segmentation

**Script**: `scripts/segmentation.ipynb`

**Input**: 
- `data/primary_data/letters/pseudon_dls/`: pseudonymised MCA and OH decision letters (.txt).

**Output**: 
- `data/primary_data/letters/segmented_dls/`: segmented pseudonymised MCA and OH decision letters (.txt).

### üîç 4. Extraction 

**Script**: `scripts/extraction.ipynb`

**Input**: 
- `data/primary_data/letters/segmented_dls/`: segmented pseudonymised MCA and OH decision letters (.txt).
- `data/models/ner/`: pre-trained convicted crime NER model.

**Output**: 
- `data/primary_data/extract/extract_data/`: MCA and OH extracted crime entities (.xlsx).

### üîÅ 5. Simplification 

**Script**: `scripts/simplification.ipynb`

**Input**: 
- `data/primary_data/extract/extract_data/`: MCA and OH extracted crime entities (.xlsx).
- `data/models/offence_cat/`: pre-trained offence category classification model (.pkl).
- `data/models/offence_type/`: pre-trained MCA and OH offence type classification models (.pkl).

**Output**: 
- `data/primary_data/extract/simplified_data/`: simplified MCA and OH extracted crime entities (.xlsx).

### üîó 6. Linkage 

**Script**: `scripts/linkage.ipynb`

**Input**: 
- `data/primary_data/extract/simplified_data/`: simplified MCA and OH extracted crime entities (.xlsx).
- `data/primary_data/letters/segmented_dls/`: segmented pseudonymised MCA and OH decision letters (.txt).
- `data/supplementary_data/`: Parole Board administrative data (.xlsx).

**Output**: 
- `data/linked_data/linked/`: MCA and OH linked data (.xlsx).

### üõ†Ô∏è 7. Preparation

**Script**: `scripts/preparation.ipynb`

**Input**: 
- `data/linked_data/linked/`: MCA and OH linked data (.xlsx).

**Output**: 
- `data/linked_data/prepared/`: MCA and OH prepared linked data (.xlsx).

The `DocLoader` class is used within the **Pseudonymisation** and **Extraction** stages of the process. It automatically handles document processing, so there‚Äôs no need to manually run this script as part of the pipeline:

### üì¶ Document caching

**Script**: `scripts/docloader.py`

**Input**: 
- `data/primary_data/letters/original_dls/`: MCA and OH decision letters (.doc, .docx).
- `data/primary_data/letters/segmented_dls/`: segmented pseudonymised MCA and OH decision letters (.txt).

**Output**: 
- `data/primary_data/letters/caches/`: Cached versions of the processed MCA and OH decision letters (.spacy).

**NB.** The original code for this project was developed in a secure environment and run on over 20,000 letters, allowing for thousands of letter types to be tested. The code in this repository is a refactored version of the original code, as it was rewritten outside of the secure environment. Therefore, there may be some inconsistencies between the original code and this refactored version.

_If you run this code and come across any mistakes, please let me know, and I will retrieve the original copy._

## Project

The scripts and data were created during a three-year PhD project. The entire process is documented in a thesis, where specific sections correspond to each of the scripts:

Chapter 5.2: _Document caching_ - `docloader.py`  
Chapter 5.3: _Decision letter classification_ - `dl_classification.R`  
Chapter 5.4: _Pseudonymisation_ - `pseudonymisation.ipynb`  
Chapter 6.2: _Letter segmentation_ - `segmentation.ipynb`  
Chapter 6.3: _Extraction_ - `extraction.ipynb`  
Chapter 7.2-7.4: _Data simplification_ - `simplification.ipynb`  
Chapter 8.2: _Linkage_ - `linkage.ipynb`  
Chapter 8.3: _Sample preparation_ - `preparation.ipynb`  

For more detailed information on the development of each process and its performance during the testing stages, please refer to the [thesis document]().

## Citation

If you use the code from this repository in your work, please cite my PhD thesis:

**Kane, E.** (2025) _Turning text into data: Exploring the potential of natural language processing techniques in extracting information from Parole Board decision letters_. PhD Thesis. University of Leeds.

## License

This repository is licensed under the MIT License.

## Contact 

For questions, feel free to reach out via email: [erica.r.kane@gmail.com]().
