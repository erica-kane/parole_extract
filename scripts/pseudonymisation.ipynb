{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from docloader import DocLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pseudonymise_content(doc):\n",
    "    \"\"\"\n",
    "    Extracts and replaces names from a doc's content using SpaCy's Named Entity Recognition (NER)\n",
    "    and regular expressions for headers and signatures.\n",
    "\n",
    "    Args:\n",
    "    - doc: A SpaCy document object.\n",
    "\n",
    "    Returns:\n",
    "        str: The pseudonymised content.\n",
    "    \"\"\"\n",
    "    # List of words to ignore during name extraction\n",
    "    ignore_list = [\n",
    "        's', 'prison', 'distribution', 'and', 'now', 'known', 'as', 'formerly',\n",
    "        'aka', 'the', 'secretary', 'of', 'state', '#', '&', ',', '-', ' '\n",
    "    ]\n",
    "\n",
    "    # Convert full document text and tokens to lowercase for case-insensitive matching\n",
    "    full_text_lower = doc.text.lower()\n",
    "    tokens_lower = [token.lower_ for token in doc]\n",
    "\n",
    "    # Extract names using SpaCy NER, remove unnecesssary punctuation, and filter out names in ignore_list\n",
    "    ner_names = [token.text.lower() for token in doc if token.ent_type_ == 'PERSON']\n",
    "    ner_names = [name.replace('(', '').replace(')', '').strip() for name in ner_names]\n",
    "    ner_names_filtered = [name for name in ner_names if name not in ignore_list]\n",
    "\n",
    "    # Extract names from headers using regular expressions, remove unnecesssary punctuation, and filter out names in ignore_list\n",
    "    header_match = re.search(r'name:(.*)\\n', full_text_lower)\n",
    "    if header_match:\n",
    "        header_text = header_match.group(1).replace('-', ' ').replace('/', ' ')\n",
    "        header_names_filtered = [\n",
    "            name.strip().replace(',', '').replace('(', '').replace(')', '') \n",
    "            for name in header_text.split() if name not in ignore_list\n",
    "        ]\n",
    "\n",
    "    # Extract names from signatures using regular expressions, remove unnecesssary punctuation, and filter out names in ignore_list\n",
    "    sig_pattern_1 = re.search(r'parole board(?: member)?:(.*)\\n', full_text_lower)\n",
    "    sig_pattern_2 = re.search(r'parole board(?::)?(.*)(\\s*)distribution', full_text_lower)\n",
    "    \n",
    "    if sig_pattern_1 or sig_pattern_2:\n",
    "        sig_text = (sig_pattern_1 or sig_pattern_2).group(1).replace('-', ' ').replace('/', ' ')\n",
    "        sig_names_filtered = [\n",
    "            name.strip().replace(',', '').replace('(', '').replace(')', '') \n",
    "            for name in sig_text.split() if name not in ignore_list\n",
    "        ]\n",
    "\n",
    "    # Combine all extracted names (NER, headers, signatures) and remove duplicates by placing them into a set\n",
    "    all_names = set(ner_names_filtered + header_names_filtered + sig_names_filtered)\n",
    "\n",
    "    # Create an ordered list of names for replacememnt \n",
    "    ordered_names = []\n",
    "    for token in tokens_lower:\n",
    "        if token in all_names and token not in ordered_names:\n",
    "            ordered_names.append(token)\n",
    "\n",
    "    # Replace names with anonymous placeholders\n",
    "    anonymised_tokens = []\n",
    "    for token in doc:\n",
    "        token_lower = token.text.lower()\n",
    "        # Do not replace single character strings\n",
    "        if token_lower in ordered_names and len(token.text) > 1:\n",
    "            placeholder = 'X' + str(ordered_names.index(token_lower))\n",
    "            anonymised_tokens.append(placeholder)\n",
    "        else:\n",
    "            anonymised_tokens.append(token.text)\n",
    "        anonymised_tokens.append(token.whitespace_)\n",
    "\n",
    "    # Reconstruct and return the anonymised text\n",
    "    pseudonymised_text = ''.join(anonymised_tokens)\n",
    "    \n",
    "    return pseudonymised_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pseudonymise_letters(letter_paths, folder_path_pseudon, loader):\n",
    "    \"\"\"\n",
    "    Pseudonymises the content per letter from a list of letter paths and saves the content in a pseudon folder.\n",
    "\n",
    "    Args:\n",
    "    - letter_paths (list): List of letter paths to be pseudonymised.\n",
    "    - folder_path_pseudon (str): Path to the folder where pseudonymised letters will be saved.\n",
    "    - loader (DocLoader): A DocLoader instance to load documents.\n",
    "    \"\"\"\n",
    "    folder_pseudon = Path(folder_path_pseudon)\n",
    "\n",
    "    for letter_path in letter_paths:\n",
    "        letter_name = letter_path.name\n",
    "        # Load the doc\n",
    "        doc = loader.load_pseudon(letter_path)\n",
    "        \n",
    "        # Pseudonymise the doc\n",
    "        letter_content_pseudon = pseudonymise_content(doc)\n",
    "        \n",
    "        # Save the pseudonymised content as a .txt file\n",
    "        with open((folder_pseudon / letter_name).with_suffix('.txt'), 'w', encoding='utf-8') as letter_pseudon:\n",
    "            letter_pseudon.write(letter_content_pseudon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pseudonymisation(loader, folder_path_pseudon):\n",
    "    '''\n",
    "    Runs the pseudonymisation process on letters that haven't been pseudonymised yet.\n",
    "\n",
    "    Args:\n",
    "    - loader (DocLoader): A DocLoader instance to load docs.\n",
    "    - folder_path_pseudon (str): Path to the folder where pseudonymised letters will be stored.\n",
    "    '''\n",
    "    # Load all paths from the letter folder\n",
    "    all_letter_paths = set(loader.all_letter_paths_pseudon())\n",
    "    # Get all paths that have already been pseudonymised\n",
    "    already_pseudonymised_letter_paths = set(Path(folder_path_pseudon).glob('*.txt'))\n",
    "    \n",
    "    # Only process new letters (those not already pseudonymised)\n",
    "    letter_paths = all_letter_paths - already_pseudonymised_letter_paths\n",
    "    \n",
    "    # Perform the pseudonymisation process\n",
    "    pseudonymise_letters(letter_paths, folder_path_pseudon, loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the SpaCy model\n",
    "nlp = spacy.load('en_core_web_trf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths for letters, caches, and pseudonymised folders\n",
    "folder_path_letters_mcadl = '../data/primary_data/letters/mcadl/original_dls/'\n",
    "folder_path_letters_ohdl = '../data/primary_data/letters/ohdl/original_dls/'\n",
    "\n",
    "folder_path_cache_mcadl = '../data/primary_data/letters/mcadl/caches/en_core_web_trf_pseudon'\n",
    "folder_path_cache_ohdl = '../data/primary_data/letters/ohdl/caches/en_core_web_trf_pseudon'\n",
    "\n",
    "folder_path_pseudon_mcadl = '../data/primary_data/letters/mcadl/pseudon_dls/'\n",
    "folder_path_pseudon_ohdl = '../data/primary_data/letters/ohdl/pseudon_dls/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the DocLoader instances\n",
    "loader_mcadl = DocLoader(nlp, folder_path_letters_mcadl, folder_path_cache_mcadl)\n",
    "loader_ohdl = DocLoader(nlp, folder_path_letters_ohdl, folder_path_cache_ohdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the pseudonymisation process on all new documents\n",
    "run_pseudonymisation(loader_mcadl, folder_path_pseudon_mcadl)\n",
    "run_pseudonymisation(loader_ohdl, folder_path_pseudon_ohdl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
