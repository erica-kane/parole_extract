{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import string\n",
    "import numpy as np\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mcadl(letter_path):\n",
    "    \"\"\"\n",
    "    Extracts relevant details from a letter of type MCADL. \n",
    "    The function looks for prisoner number, hearing date, and decision details within the file.\n",
    "\n",
    "    Args:\n",
    "    - letter_path (str): Path to the letter file.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Dictionary containing extracted details (letter_id, prisoner_id, hearing date, and decision).\n",
    "    \"\"\"\n",
    "    # Extract letter name (without extension)\n",
    "    letter_name = Path(letter_path).stem\n",
    "\n",
    "    # Open and read the letter content\n",
    "    with open(letter_path, 'r', encoding='utf8') as letter:\n",
    "        letter_content = letter.read().lower()  # Process content in lowercase for easier regex matching\n",
    "\n",
    "    # Extract prison ID\n",
    "    prison_id_match = re.search(r'number:(.*)\\n', letter_content)\n",
    "    prison_id = prison_id_match.group(1).strip() if prison_id_match else 'error'\n",
    "\n",
    "    # Remove punctuation from prison ID\n",
    "    prison_id_punc = re.sub(r'[' + re.escape(string.punctuation) + ']', ' ', prison_id)\n",
    "\n",
    "    # Replace multiple spaces with a single space\n",
    "    prison_id_space = re.sub(r'\\s+', ' ', prison_id_punc)\n",
    "\n",
    "    # Split the cleaned ID into a list and remove unwanted terms\n",
    "    prison_ids_list = prison_id_space.split()\n",
    "    ignore_list = ['scottish', 'prison', 'number']\n",
    "    prison_ids_list_final = [id for id in prison_ids_list if id not in ignore_list]\n",
    "\n",
    "    # Extract hearing date\n",
    "    hearing_date_match = re.search(r'date:(.*)\\n', letter_content)\n",
    "    hearing_date = hearing_date_match.group(1).strip() if hearing_date_match else 'error'\n",
    "\n",
    "    # Normalise date by replacing different separators with '/'\n",
    "    hearing_date_sep = re.sub(r'\\s+', '', hearing_date).replace('-', '/').replace('.', '/')\n",
    "\n",
    "    # Validate and standardise date format\n",
    "    hearing_date_final = hearing_date_sep if re.match(r'\\b\\d{2}/\\d{2}/\\d{4}\\b', hearing_date_sep) else 'error'\n",
    "\n",
    "    # Extract decision\n",
    "    decision_match = re.search(r'decision\\s*:(.*)\\n', letter_content)\n",
    "    decision_final = decision_match.group(1).strip() if decision_match else 'error'\n",
    "\n",
    "    # Return extracted data as a dictionary\n",
    "    return {\n",
    "        'letter_id': letter_name,\n",
    "        'prisoner_id': prison_ids_list_final,\n",
    "        'date': hearing_date_final,\n",
    "        'decision': decision_final\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ohdl(letter_path):\n",
    "    \"\"\"\n",
    "    Extract relevant details from a letter file, including prisoner ID(s) and hearing date(s).\n",
    "    \n",
    "    Args:\n",
    "    - letter_path (str): Path to the letter file.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: A dictionary containing the letter name, a list of prisoner IDs, and a list of hearing dates.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Save letter name (without extension)\n",
    "    letter_name = Path(letter_path).stem\n",
    "\n",
    "    # Open the letter and read its content\n",
    "    with open(letter_path, 'r', encoding='utf8') as letter:\n",
    "        letter_content = letter.read().lower()  # Read and convert content to lowercase for easier searching\n",
    "    \n",
    "    # Extract prison ID\n",
    "    prison_id_match = re.search(r'number:(.*)\\n', letter_content)\n",
    "    prison_id = prison_id_match.group(1).strip() if prison_id_match else 'error'\n",
    "    \n",
    "    # Clean prison ID: remove punctuation and normalize whitespace\n",
    "    prison_id_punc = re.sub(r'[' + re.escape(string.punctuation) + ']', ' ', prison_id)  # Remove punctuation\n",
    "    prison_id_space = re.sub(r'\\s+', ' ', prison_id_punc).strip()  # Normalise whitespace\n",
    "    \n",
    "    # Convert cleaned prison IDs to a list of IDs (split by spaces)\n",
    "    prison_ids_list = prison_id_space.split()\n",
    "    \n",
    "    # Extract hearing date(s)\n",
    "    months = ['january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'october', 'november', 'december']\n",
    "    months_pattern = '|'.join(months)\n",
    "    \n",
    "    # Try different patterns to find a hearing date\n",
    "    hearing_date_match_1 = re.search(r'panel dates?:?(.*)\\n', letter_content)\n",
    "    hearing_date_match_2 = re.search(r'dates? of (panel|hearing):?(.*)\\n', letter_content)\n",
    "    hearing_date_match_3 = re.findall(r'\\b\\d{1,2}/\\d{1,2}/\\d{2,4}\\b', letter_content)\n",
    "    hearing_date_match_4 = re.findall(r'(\\d{1,2}(?:st|nd|rd|th)? (' + months_pattern + ') \\d{2,4})', letter_content)\n",
    "\n",
    "    # Extract hearing date based on the matched pattern\n",
    "    if hearing_date_match_1:\n",
    "        hearing_date = hearing_date_match_1.group(1).strip()\n",
    "    elif hearing_date_match_2:\n",
    "        hearing_date = hearing_date_match_2.group(2).strip()\n",
    "    elif hearing_date_match_3:\n",
    "        hearing_date = hearing_date_match_3[0]  # Take the first matching date\n",
    "    elif hearing_date_match_4:\n",
    "        hearing_date = hearing_date_match_4[0][0]  # Take the first matching \"dd month yyyy\" pattern\n",
    "    else:\n",
    "        hearing_date = 'error'\n",
    "\n",
    "    # Standardize hearing date format\n",
    "    # Replace different separators with '/' for consistency\n",
    "    hearing_date_sep = hearing_date.replace('-', '/').replace('.', '/').replace('and', ' ').replace('&', ' ').strip()\n",
    "\n",
    "    # Convert \"dd month yyyy\" format to \"dd/mm/yyyy\" using datetime conversion\n",
    "    if re.match(r'(\\d{1,2}(?:st|nd|rd|th)? (' + months_pattern + ') \\d{2,4})', hearing_date_sep):\n",
    "        hearing_date_sep = re.sub(r'(\\d+)(st|nd|rd|th)', r'\\1', hearing_date_sep)  # Remove day suffixes\n",
    "        try:\n",
    "            hearing_date_sep = pd.to_datetime(hearing_date_sep, format='%d %B %Y')  # Convert to datetime object\n",
    "            hearing_date_sep = hearing_date_sep.strftime('%d/%m/%Y')  # Format date as dd/mm/yyyy\n",
    "        except ValueError:\n",
    "            pass  # In case the date format isn't valid, keep it as-is\n",
    "\n",
    "    # If still in dd/mm/yy or similar, normalise it\n",
    "    if re.match(r'\\b\\d{1,2}/\\d{1,2}/\\d{2,4}\\b', hearing_date_sep):\n",
    "        hearing_date_list = []\n",
    "        hearing_date_check_1 = re.findall(r'\\b\\d{1,2}/\\d{1,2}/\\d{2,4}\\b', hearing_date_sep)\n",
    "        for hearing_date in hearing_date_check_1:\n",
    "            hearing_date_parts = hearing_date.split('/')\n",
    "            # Ensure day and month are 2 digits and year is 4 digits\n",
    "            day = hearing_date_parts[0].zfill(2)\n",
    "            month = hearing_date_parts[1].zfill(2)\n",
    "            year = '20' + hearing_date_parts[2] if len(hearing_date_parts[2]) == 2 else hearing_date_parts[2]\n",
    "            hearing_date_normal = f'{day}/{month}/{year}'\n",
    "            hearing_date_list.append(hearing_date_normal)\n",
    "    else:\n",
    "        hearing_date_list = hearing_date_sep\n",
    "\n",
    "    # Handle multiple hearing dates\n",
    "    # If hearing_date_sep is a string with multiple dates (e.g., \"12 June 2022 and 14 June 2022\"), extract and clean all dates\n",
    "    if isinstance(hearing_date_list, str):\n",
    "        hearing_date_check_2 = re.findall(r'(\\d{1,2}(?:st|nd|rd|th)? (' + months_pattern + ') \\d{2,4})', hearing_date_sep)\n",
    "        if hearing_date_check_2:\n",
    "            hearing_dates_list = []\n",
    "            for hearing_date in hearing_date_check_2:\n",
    "                hearing_date_suffix = re.sub(r'(\\d+)(st|nd|rd|th)', r'\\1', hearing_date[0])  # Remove suffixes\n",
    "                hearing_date_parsed = pd.to_datetime(hearing_date_suffix, format='%d %B %Y')\n",
    "                hearing_date_format = hearing_date_parsed.strftime('%d/%m/%Y')\n",
    "                hearing_dates_list.append(hearing_date_format)\n",
    "        else:\n",
    "            hearing_dates_list = hearing_date_list\n",
    "    else:\n",
    "        hearing_dates_list = hearing_date_list\n",
    "\n",
    "    # Return the extracted data in a structured dictionary\n",
    "    return {\n",
    "        'letter_id': letter_name, \n",
    "        'prisoner_id': prison_ids_list, \n",
    "        'date': hearing_dates_list\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_extraction(folder_path_segment, simplified_data, letter_type):\n",
    "    \"\"\"\n",
    "    Extracts data from a set of letters (MCADL or OHDL) and merges it with an existing dataframe.\n",
    "\n",
    "    Args:\n",
    "    - folder_path_segment (str): Path to the folder containing letter files.\n",
    "    - simplified_data (pd.DataFrame): Dataframe with additional details to merge with extracted data.\n",
    "    - letter_type (str): Type of letter to extract data from ('mcadl' or 'ohdl').\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Merged dataframe with extracted details and additional data.\n",
    "    \"\"\"\n",
    "    extract_results_list = []  # List to store extracted data\n",
    "    letter_paths = list(Path(folder_path_segment).glob('*.txt*'))  # Gather all text files from the folder\n",
    "\n",
    "    # Process each letter based on type and extract data\n",
    "    for letter_path in letter_paths:\n",
    "        if letter_type == 'mca':\n",
    "            extract_result = extract_mcadl(letter_path)\n",
    "        elif letter_type == 'oh':\n",
    "            extract_result = extract_ohdl(letter_path)\n",
    "        extract_results_list.append(extract_result)\n",
    "\n",
    "    # Convert extracted data into a dataframe\n",
    "    extract_results_data = pd.DataFrame.from_records(extract_results_list)\n",
    "\n",
    "    # Merge extracted data with the provided simplified dataframe\n",
    "    extract_data = pd.merge(extract_results_data, simplified_data, on='letter_id', how='inner')\n",
    "\n",
    "    return extract_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping for representation status\n",
    "rep_dict = {\n",
    "    'Not Represented': 'valid',\n",
    "    'Represented': 'valid'\n",
    "}\n",
    "\n",
    "# Mapping for hearing types\n",
    "hearing_type_dict = {\n",
    "    'MCA Paper Hearing': 'mca',\n",
    "    'Paper - Single Member': 'mca',\n",
    "    'Oral Hearing': 'oh',\n",
    "    'OH to Conclude on Papers': 'oh',\n",
    "    'Duty Member Oral Hearing Request': 'oh'\n",
    "}\n",
    "\n",
    "# Mapping for decision descriptions\n",
    "decision_dict = {\n",
    "    'release at a future date': 'direction for release',\n",
    "    'directed to oral hearing': 'directed to oral hearing',\n",
    "    'remain in custody (knockback)(so) [*]': 'no direction for release',\n",
    "    'open condition (so) [*]': 'open conditions',\n",
    "    'no recommendation for open': 'no open conditions',\n",
    "    'paper deferred': 'deferral/adjounment',\n",
    "    'paper adjourned': 'deferral/adjounment',\n",
    "    'release (so) [**]': 'direction for release',\n",
    "    'not granted': 'no direction for release',\n",
    "    'cancelled - sed passed': 'cancellation/withdrawal',\n",
    "    'oh adjourned': 'deferral/adjounment',\n",
    "    'oh deferral': 'deferral/adjounment',\n",
    "    'withdrawn by ppcs - executive release': 'cancellation/withdrawal',\n",
    "    'oral hearing - parole board (so) [*]': 'direct to oh',\n",
    "    'recommend open conditions': 'open conditions',\n",
    "    'no direction for release - deferral': 'no direction for release', \n",
    "    'immediate release. oral hearing not granted': 'direction for release',\n",
    "    'no direction x8 x9': 'no direction for release',\n",
    "    'recommend open conditions': 'open conditions',\n",
    "    'no recommendation for open conditions': 'no open conditions',\n",
    "    'release (so) [*]': 'direction for release',\n",
    "    'direct release': 'direction for release',\n",
    "    'adjourned - on the day': 'deferral/adjounment',\n",
    "    'oh to conclude on papers': 'direct to oh',\n",
    "    'directed to oral hearing': 'direct to oh',\n",
    "    'recommend release': 'direction for release',\n",
    "    'adjourned - before the hearing': 'deferral/adjounment',\n",
    "    'sent to a directions hearings': 'direct to oh',\n",
    "    'proceeding to a directions hearing': 'direct to oh',\n",
    "    'withdrawn by ppcs - mh case': 'cancellation/withdrawal',\n",
    "    'deferred': 'deferral/adjounment',\n",
    "    'sent to multi member panel': 'direct to oh',\n",
    "    'withdrawn by ppcs - deportation': 'cancellation/withdrawal',\n",
    "    'withdrawn by ppcs - death in custody': 'cancellation/withdrawal',\n",
    "    'no hearing sed within 20 weeks': 'cancellation/withdrawal',\n",
    "    'sent back to mca to conclude on papers': 'cancellation/withdrawal',\n",
    "    'other': np.nan,\n",
    "    'not specified': np.nan,\n",
    "    'not applicable': np.nan\n",
    "}\n",
    "\n",
    "# Mapping for decision validity\n",
    "decision_valid_dict = {\n",
    "    'no direction for release': 'valid',\n",
    "    'direction for release': 'valid',\n",
    "    'open conditions': 'valid',\n",
    "    'no open conditions': 'valid',\n",
    "    'direct to oh': 'not valid',\n",
    "    'deferral/adjounment': 'not valid',\n",
    "    'cancellation/withdrawal': 'not valid'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_prisoner_id(prisoner_id_str):\n",
    "    \"\"\"\n",
    "    Splits a prisoner ID string into individual components, removing any whitespace or punctuation.\n",
    "\n",
    "    Args:\n",
    "    - prisoner_id_str (str): The prisoner ID string to split.\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of prisoner ID components after splitting and cleaning.\n",
    "            If input is not a string, returns 'error'.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compile a regex pattern to match various delimiters (/, (), spaces, and -)\n",
    "    pattern = re.compile(r'\\s*(?:/|\\(|\\)|\\s+|-)s*')\n",
    "\n",
    "    # Ensure the input is a string before processing\n",
    "    if isinstance(prisoner_id_str, str):\n",
    "        # Split the string based on the pattern and filter out empty strings\n",
    "        split_string = pattern.split(prisoner_id_str)\n",
    "        clean_string = [s for s in split_string if s.strip()]\n",
    "    else:\n",
    "        clean_string = 'error'\n",
    "    \n",
    "    return clean_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicate_filtering(dupe_data):\n",
    "    \"\"\"\n",
    "    Filters duplicates from the input DataFrame based on decision and representation validity.\n",
    "\n",
    "    Args:\n",
    "    - dupe_data (pd.DataFrame): DataFrame containing duplicates with 'prisoner_id', 'decision', and 'representation_status_description'.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Filtered DataFrame with duplicates resolved.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    dupe_copy = dupe_data.copy()\n",
    "\n",
    "    # Add a new column 'rep_valid' to indicate if the representation status is valid\n",
    "    dupe_copy['rep_valid'] = dupe_copy['representation_status_description'].replace(rep_dict)\n",
    "\n",
    "    # Get the unique prisoner IDs from the DataFrame\n",
    "    prisoner_ids = set(dupe_copy['prisoner_id'].unique())\n",
    "\n",
    "    # Create an empty DataFrame to store the final filtered results\n",
    "    relevant_data = pd.DataFrame(columns=dupe_copy.columns)\n",
    "    \n",
    "    # Iterate over each prisoner ID and filter rows based on decision and representation status\n",
    "    for prisoner_id in prisoner_ids:\n",
    "        \n",
    "        subset_data = dupe_copy[dupe_copy['prisoner_id'] == prisoner_id]\n",
    "        subset_data_decisions = set(subset_data['decision_valid'])\n",
    "        subset_data_rep = set(subset_data['rep_valid'])\n",
    "        \n",
    "        # Multiple decision statuses present\n",
    "        if len(subset_data_decisions) != 1:\n",
    "            \n",
    "            # Check if any decision is NaN\n",
    "            if subset_data[subset_data['decision'].isna()].empty:\n",
    "                \n",
    "                # Prioritize valid decisions\n",
    "                if 'valid' in subset_data_decisions:\n",
    "                    relevant_row = subset_data[subset_data['decision_valid'] == 'valid']\n",
    "\n",
    "                # Handle mixed representation statuses\n",
    "                elif len(subset_data_rep) != 1 and 'valid' in subset_data_rep:\n",
    "                    relevant_row = subset_data[subset_data['rep_valid'] == 'valid']\n",
    "                    \n",
    "            else:\n",
    "                # If no valid decision found, filter by non-NaN decisions\n",
    "                relevant_row = subset_data[subset_data['decision'].notna()]\n",
    "        \n",
    "        else:\n",
    "            # If there's only one decision status, filter based on representation validity\n",
    "            if len(subset_data_rep) != 1 and 'valid' in subset_data_rep:\n",
    "                relevant_row = subset_data[subset_data['rep_valid'] == 'valid']\n",
    "        \n",
    "        # Concatenate the filtered rows into the final DataFrame\n",
    "        relevant_data = pd.concat([relevant_data, relevant_row])\n",
    "\n",
    "    # Drop the temporary 'rep_valid' column\n",
    "    return relevant_data.drop(['rep_valid'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_supp(supp_data):\n",
    "    \"\"\"\n",
    "    Prepares supplementary hearing data by cleaning, transforming, and removing duplicates.\n",
    "\n",
    "    Args:\n",
    "    - supp_data (pd.DataFrame): The input DataFrame containing supplementary hearing data.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The cleaned and prepared DataFrame with duplicates removed.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create copies to avoid modifying original data\n",
    "    supp_copy = supp_data.copy()\n",
    "\n",
    "    # Data transformation and cleaning\n",
    "    supp_copy['prisoner_id'] = supp_copy['PRISON_NUMBER'].str.lower()\n",
    "    supp_copy['hearing_date'] = pd.to_datetime(supp_copy['HEARING_START_DATE'])\n",
    "    supp_copy['hearing_type'] = supp_copy['HEARING_TYPE_DESCRIPTION'].replace(hearing_type_dict)\n",
    "    supp_copy['decision'] = supp_copy['HEARING_RESULT_DESCRIPTION'].str.lower().replace(decision_dict)\n",
    "    supp_copy['decision_valid'] = supp_copy['decision'].replace(decision_valid_dict)\n",
    "\n",
    "    # Rename columns to lowercase and select relevant columns\n",
    "    supp_copy.columns = supp_copy.columns.str.lower()\n",
    "    supp_copy = supp_copy[['prisoner_id', 'hearing_date', 'current_establishment_description', 'custody_type_description', \n",
    "                           'ethnicity_description', 'gender', 'decision', 'decision_valid', 'nationality_description',\n",
    "                           'original_target_date', 'review_reason_description', 'hearing_type', 'representation_status_description',\n",
    "                           'years_old_at_hearing']]\n",
    "    \n",
    "    # Remove duplicates\n",
    "    supp_copy = supp_copy.drop_duplicates(keep='first').reset_index(drop=True)\n",
    "\n",
    "    # Apply prisoner ID splitting and expand rows for multiple IDs\n",
    "    supp_copy['prisoner_id'] = supp_copy['prisoner_id'].apply(split_prisoner_id)\n",
    "    supp_copy_long = supp_copy.explode('prisoner_id')\n",
    "    supp_copy_long = supp_copy_long[supp_copy_long['prisoner_id'] != 'error'].reset_index(drop=True)\n",
    "\n",
    "    # Handle duplicates\n",
    "    supp_copy_duplicates = supp_copy_long[supp_copy_long.duplicated(subset=('prisoner_id', 'hearing_date'), keep=False)]\n",
    "    supp_copy_no_duplicates = supp_copy_long.drop_duplicates(subset=('prisoner_id', 'hearing_date'), keep=False)\n",
    "    supp_copy_correct_duplicates = duplicate_filtering(supp_copy_duplicates)\n",
    "    prepared_supp_data = pd.concat([supp_copy_no_duplicates, supp_copy_correct_duplicates]).reset_index(drop=True)\n",
    "\n",
    "    return prepared_supp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_extract_mcadl(extract_data_mcadl, prepared_supp_data):\n",
    "    \"\"\"\n",
    "    Prepares the extracted MCA hearing data by mapping decisions, formatting dates, and filtering relevant prisoner records.\n",
    "\n",
    "    Args:\n",
    "    - extract_data_mcadl (pd.DataFrame): The extracted dataset containing prisoner hearing information.\n",
    "    - prepared_supp_data (pd.DataFrame): The supplementary dataset with pre-processed MCA hearing data.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The prepared extract data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter the supplementary data for MCA hearings\n",
    "    prepared_supp_data_mcadl = prepared_supp_data[prepared_supp_data['hearing_type'] == 'mca']\n",
    "    prepared_supp_data_prisoner_ids = set(prepared_supp_data_mcadl['prisoner_id'])  # Set of prisoner IDs in supplementary MCA data\n",
    "\n",
    "    # Create a copy of the extract data to avoid modifying the original\n",
    "    extract_copy_mcadl = extract_data_mcadl.copy()\n",
    "\n",
    "    # Remove rows with invalid date values and convert dates to datetime format\n",
    "    extract_copy_mcadl = extract_copy_mcadl[extract_copy_mcadl['date'] != 'error'].reset_index(drop=True)\n",
    "    extract_copy_mcadl['hearing_date'] = pd.to_datetime(extract_copy_mcadl['date'], format='%d/%m/%Y')\n",
    "    extract_copy_mcadl = extract_copy_mcadl.drop('date', axis=1)  # Drop original 'date' column after conversion\n",
    "\n",
    "    # Explode any lists in the 'prisoner_id' column into separate rows\n",
    "    extract_copy_long_mcadl = extract_copy_mcadl.explode('prisoner_id').reset_index(drop=True)\n",
    "\n",
    "    # Map the decision descriptions to standardized values\n",
    "    extract_copy_long_mcadl['decision'] = extract_copy_long_mcadl['decision'].str.lower().replace(decision_dict)\n",
    "    # Add a column to indicate if the decision is valid\n",
    "    extract_copy_long_mcadl['decision_valid'] = extract_copy_long_mcadl['decision'].replace(decision_valid_dict)\n",
    "\n",
    "    # Drop duplicate rows based on 'prisoner_id' and 'hearing_date'\n",
    "    extract_copy_long_mcadl = extract_copy_long_mcadl.drop_duplicates(subset=('prisoner_id', 'hearing_date'), keep='first').reset_index(drop=True)\n",
    "\n",
    "    # Get the set of prisoner IDs from the extract data\n",
    "    extract_copy_prisoner_ids_mcadl = set(extract_copy_long_mcadl['prisoner_id'])\n",
    "\n",
    "    # Find the overlapping prisoner IDs between extract data and supplementary MCA data\n",
    "    overlapping_prisoner_ids = [prisoner_id for prisoner_id in extract_copy_prisoner_ids_mcadl if prisoner_id in prepared_supp_data_prisoner_ids]\n",
    "\n",
    "    # Filter the extract data to include only relevant records with overlapping prisoner IDs\n",
    "    prepared_extract_data_mcadl = extract_copy_long_mcadl[extract_copy_long_mcadl['prisoner_id'].isin(overlapping_prisoner_ids)].reset_index(drop=True)\n",
    "\n",
    "    return prepared_extract_data_mcadl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_extract_ohdl(extract_data_ohdl, prepared_supp_data):\n",
    "    \"\"\"\n",
    "    Prepares the extracted Oral Hearing (OH) data by formatting dates and filtering relevant prisoner records.\n",
    "\n",
    "    Args:\n",
    "    - extract_data_ohdl (pd.DataFrame): The extracted dataset containing prisoner hearing information.\n",
    "    - prepared_supp_data (pd.DataFrame): The supplementary dataset.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The relevant extracted data containing OH hearings.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter the supplementary data for Oral Hearings (OH)\n",
    "    prepared_supp_data_ohdl = prepared_supp_data[prepared_supp_data['hearing_type'] == 'oh']\n",
    "    prepared_supp_data_prisoner_ids = set(prepared_supp_data_ohdl['prisoner_id'])  # Set of prisoner IDs in supplementary OH data\n",
    "\n",
    "    # Create a copy of the extract data to avoid modifying the original\n",
    "    extract_copy_ohdl = extract_data_ohdl.copy()\n",
    "\n",
    "    # Drop rows with missing dates and reset the index\n",
    "    extract_copy_ohdl = extract_copy_ohdl.dropna(subset=['date']).reset_index(drop=True)\n",
    "    extract_copy_ohdl = extract_copy_ohdl[extract_copy_ohdl['date'] != 'error'].reset_index(drop=True)\n",
    "\n",
    "    # Explode any lists in the 'date' column into separate rows\n",
    "    extract_copy_long_date_ohdl = extract_copy_ohdl.explode('date').reset_index(drop=True)\n",
    "    # Convert the date strings to datetime format\n",
    "    extract_copy_long_date_ohdl['hearing_date'] = pd.to_datetime(extract_copy_long_date_ohdl['date'], format='%d/%m/%Y')\n",
    "    extract_copy_long_date_ohdl = extract_copy_long_date_ohdl.drop('date', axis=1)  # Drop original 'date' column\n",
    "\n",
    "    # Explode any lists in the 'prisoner_id' column into separate rows\n",
    "    extract_copy_long_id_ohdl = extract_copy_long_date_ohdl.explode('prisoner_id').reset_index(drop=True)\n",
    "    # Filter out rows where 'prisoner_id' is marked as 'error'\n",
    "    extract_copy_long_id_ohdl = extract_copy_long_id_ohdl[extract_copy_long_id_ohdl['prisoner_id'] != 'error']\n",
    "\n",
    "    # Drop duplicate rows based on 'prisoner_id' and 'hearing_date'\n",
    "    extract_copy_long_id_ohdl = extract_copy_long_id_ohdl.drop_duplicates(subset=('prisoner_id', 'hearing_date'), keep='first').reset_index(drop=True)\n",
    "\n",
    "    # Get the set of prisoner IDs from the extract data\n",
    "    extract_copy_prisoner_ids_ohdl = set(extract_copy_long_id_ohdl['prisoner_id'])\n",
    "\n",
    "    # Find the overlapping prisoner IDs between extract data and supplementary OH data\n",
    "    overlapping_prisoner_ids = [prisoner_id for prisoner_id in extract_copy_prisoner_ids_ohdl if prisoner_id in prepared_supp_data_prisoner_ids]\n",
    "\n",
    "    # Filter the extract data to include only relevant records with overlapping prisoner IDs\n",
    "    prepared_extract_data_ohdl = extract_copy_long_id_ohdl[extract_copy_long_id_ohdl['prisoner_id'].isin(overlapping_prisoner_ids)].reset_index(drop=True)\n",
    "\n",
    "    return prepared_extract_data_ohdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_extract(extract_data, prepared_supp_data, letter_type):\n",
    "    \"\"\"\n",
    "    Prepares the extracted data based on the letter type.\n",
    "\n",
    "    This function selects the appropriate preparation method depending on \n",
    "    the given `letter_type` and processes the `extract_data`, using the `prepared_supp_data`.\n",
    "\n",
    "    Args:\n",
    "    - extract_data (pd.DataFrame): The extracted dataset containing prisoner hearing information.\n",
    "    - prepared_supp_data (pd.DataFrame): The supplementary dataset.\n",
    "    - letter_type (str): Type of the letter, can be either 'mca' or 'oh'.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The relevant extracted data for either OHDLs or MCADLs.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Determine preparation function based on letter type\n",
    "    if letter_type == 'mca':\n",
    "        # Prepare data for 'mca' letter type\n",
    "        prepared_extract_data = prepare_extract_mcadl(extract_data, prepared_supp_data)\n",
    "    elif letter_type == 'oh':\n",
    "        # Prepare data for 'oh' letter type\n",
    "        prepared_extract_data = prepare_extract_ohdl(extract_data, prepared_supp_data)\n",
    "    \n",
    "    return prepared_extract_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_row(extract_data_row, prepared_supp_data):\n",
    "    \"\"\"\n",
    "    Links a row from extract data to the relevant supplementary data based on prisoner ID and hearing date.\n",
    "    \n",
    "    Parameters:\n",
    "    - extract_data_row (pd.Series): A single row from the extract data containing prisoner information.\n",
    "    - prepared_supp_data (pd.DataFrame): The supplementary data containing relevant hearing details.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: A merged single row DataFrame containing information from both the extract and relevant supplemental data.\n",
    "    \"\"\"\n",
    "    \n",
    "    prison_id = extract_data_row['prisoner_id']  # Get the prisoner ID from the extract data row\n",
    "    hearing_date = extract_data_row['hearing_date']  # Get the hearing date from the extract data row\n",
    "\n",
    "    # Filter supplemental data for rows with the same prisoner ID\n",
    "    supp_data_relevant = prepared_supp_data[prepared_supp_data['prisoner_id'] == prison_id].copy()\n",
    "    # Calculate the absolute difference in days between the hearing dates\n",
    "    supp_data_relevant['difference_days'] = (supp_data_relevant['hearing_date'] - hearing_date).dt.days.abs()\n",
    "\n",
    "    # Check for same day with valid decisions\n",
    "    if not supp_data_relevant[(supp_data_relevant['difference_days'] == 0) & (supp_data_relevant['decision_valid'] == 'valid')].empty:\n",
    "        supp_data_relevant_row = supp_data_relevant[(supp_data_relevant['difference_days'] == 0) & (supp_data_relevant['decision_valid'] == 'valid')]\n",
    "        link_type = 'same day, valid decision'\n",
    "    \n",
    "    # Check for within 1 month valid decisions\n",
    "    elif not supp_data_relevant[(supp_data_relevant['difference_days'] < 31) & (supp_data_relevant['decision_valid'] == 'valid')].empty:\n",
    "        supp_data_relevant_rows = supp_data_relevant[(supp_data_relevant['difference_days'] < 31) & (supp_data_relevant['decision_valid'] == 'valid')]\n",
    "        supp_data_relevant_row_index = supp_data_relevant_rows['difference_days'].idxmin()\n",
    "        supp_data_relevant_row = pd.DataFrame(supp_data_relevant_rows.loc[supp_data_relevant_row_index]).transpose()\n",
    "        link_type = 'within 1 month, valid decision'\n",
    "    \n",
    "    # Check for same day with non-valid decisions\n",
    "    elif not supp_data_relevant[supp_data_relevant['difference_days'] == 0].empty:\n",
    "        supp_data_relevant_row = supp_data_relevant[supp_data_relevant['difference_days'] == 0]\n",
    "        link_type = 'same day, non valid decision'\n",
    "\n",
    "    # Default case: Find the closest day regardless of decision validity\n",
    "    else:\n",
    "        supp_data_relevant_row_index = supp_data_relevant['difference_days'].idxmin()\n",
    "        supp_data_relevant_row = pd.DataFrame(supp_data_relevant.loc[supp_data_relevant_row_index]).transpose()\n",
    "        \n",
    "        if supp_data_relevant_row['decision_valid'][supp_data_relevant_row_index] == 'valid':\n",
    "            link_type = 'closest day, valid decision'\n",
    "        else:\n",
    "            link_type = 'closest day, non valid decision'\n",
    "\n",
    "    supp_data_relevant_row['link_type'] = link_type  # Add link type to the final row\n",
    "\n",
    "    extract_data_row = pd.DataFrame(extract_data_row).transpose()  # Convert original row to DataFrame\n",
    "\n",
    "    # Merge original row with the final relevant row from supplemental data\n",
    "    linked_row = pd.merge(extract_data_row, supp_data_relevant_row, on='prisoner_id', suffixes=('_letter', '_supp'))\n",
    "\n",
    "    return linked_row  # Return the merged DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_mcadl(prepared_extract_data_mcadl, prepared_supp_data):\n",
    "    \"\"\"\n",
    "    Executes the linkage process for MCADL data by iterating through prepared extract data and linking with supplemental data.\n",
    "    \n",
    "    Parameters:\n",
    "    - prepared_extract_data_mcadl (pd.DataFrame): The extracted mcadl data prepared for linkage.\n",
    "    - prepared_supp_data (pd.DataFrame): The supplementary data containing relevant hearing details.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: The final linked DataFrame without duplicates and irrelevant link types.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Concatenate all linked rows by processing each row in prepared extract data\n",
    "    linked_rows = pd.concat([link_row(prepared_extract_data_mcadl.loc[i], prepared_supp_data) for i in range(len(prepared_extract_data_mcadl))]).reset_index(drop=True)\n",
    "    \n",
    "    # Remove duplicates from linked rows based on 'letter_id'\n",
    "    linked_rows_no_duplicates = linked_rows.drop_duplicates(subset=('letter_id'), keep=False)\n",
    "    linked_rows_columns = linked_rows.columns.difference(['prisoner_id'])  # Identify duplicate columns\n",
    "    linked_row_duplicates = linked_rows[linked_rows.duplicated(subset=linked_rows_columns, keep='first')]  # Find duplicate letter IDs\n",
    "\n",
    "    # Concatenate linked rows without duplicates with duplicated rows\n",
    "    linked_rows_relevant = pd.concat([linked_rows_no_duplicates, linked_row_duplicates]).reset_index(drop=True)\n",
    "\n",
    "    # Define link types to exclude from final data\n",
    "    link_types_exclude = ['closest day, non valid decision', 'same day, non valid decision', 'closest day, valid decision']\n",
    "\n",
    "    # Filter out excluded link types from the final data\n",
    "    linked_data = linked_rows_relevant.drop(linked_rows_relevant[linked_rows_relevant['link_type'].isin(link_types_exclude)].index).reset_index(drop=True)\n",
    "    # Drop unnecessary columns from final data\n",
    "    linked_data = linked_data.drop(['decision_letter', 'decision_valid_letter', 'hearing_date_supp', 'decision_valid_supp'], axis=1)\n",
    "    # Rename decision and date columns\n",
    "    linked_data.rename(columns={'decision_supp': 'decision', 'hearing_date_letter': 'hearing_date'}, inplace=True)\n",
    "\n",
    "    return linked_data  # Return the final linked DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_idx(relevant_rows):\n",
    "    \"\"\"\n",
    "    Matches relevant rows based on various rules to determine the best row for linkage.\n",
    "    \n",
    "    Parameters:\n",
    "    - relevant_rows (pd.DataFrame): A DataFrame containing potential matching relevant rows.\n",
    "    \n",
    "    Returns:\n",
    "    - tuple: The index of the chosen candidate and the applied rule.\n",
    "    \"\"\"\n",
    "    \n",
    "    relevant_rows_copy = relevant_rows.copy()\n",
    "    relevant_rows_copy['difference_days'] = relevant_rows_copy['difference_days'].astype(int)  # Ensure difference_days is integer\n",
    "\n",
    "    # Check uniqueness of prisoner ID, letter date, and supplemental date\n",
    "    prisoner_id_uniq = relevant_rows_copy['prisoner_id'].nunique() == 1\n",
    "    letter_date_uniq = relevant_rows_copy['hearing_date_letter'].nunique() == 1\n",
    "    supp_date_uniq = relevant_rows_copy['hearing_date_supp'].nunique() == 1\n",
    "\n",
    "    valid_mask = relevant_rows_copy['decision_valid'] == 'valid'  # Mask for valid decisions\n",
    "    n_valid = valid_mask.sum()  # Count of valid decisions\n",
    "\n",
    "    chosen_row_idx = None  # Initialize chosen index\n",
    "    applied_rule = None  # Initialize applied rule\n",
    "\n",
    "    # Rule 1: PID same, LD diff, SD diff\n",
    "    if prisoner_id_uniq and not letter_date_uniq and not supp_date_uniq:\n",
    "        applied_rule = '1'\n",
    "        if n_valid == 0:\n",
    "            # Choose the latest date if no valid decisions\n",
    "            chosen_row_idx = relevant_rows_copy['hearing_date_ohdl'].idxmax()\n",
    "        elif n_valid == 1:\n",
    "            # Choose the single valid candidate\n",
    "            chosen_row_idx = relevant_rows_copy[valid_mask].index[0]\n",
    "        else:\n",
    "            # Choose the candidate with the lowest difference in days\n",
    "            chosen_row_idx = relevant_rows_copy['difference_days'].idxmin()\n",
    "\n",
    "    # Rule 2: PID same, LD diff, SD same\n",
    "    elif prisoner_id_uniq and not letter_date_uniq and supp_date_uniq:\n",
    "        chosen_row_idx = relevant_rows_copy['difference_days'].idxmin()  # Choose the lowest difference\n",
    "        applied_rule = '2'\n",
    "\n",
    "    # Rule 3: PID diff, LD same, SD diff\n",
    "    elif not prisoner_id_uniq and letter_date_uniq and not supp_date_uniq:\n",
    "        chosen_row_idx = relevant_rows_copy['difference_days'].idxmin()  # Choose the lowest difference\n",
    "        applied_rule = '3'\n",
    "\n",
    "    # Rule 4: PID diff, LD same, SD same\n",
    "    elif not prisoner_id_uniq and letter_date_uniq and supp_date_uniq:\n",
    "        chosen_row_idx = relevant_rows_copy.index[0]  # Choose the first index\n",
    "        applied_rule = '4'\n",
    "\n",
    "    # Rule 5: PID diff, LD diff, SD same\n",
    "    elif (not prisoner_id_uniq and not letter_date_uniq and not supp_date_uniq) or (not prisoner_id_uniq and not letter_date_uniq and supp_date_uniq):\n",
    "        applied_rule = '5'\n",
    "        if n_valid > 1 and relevant_rows_copy['difference_days'].nunique() == 1:\n",
    "            chosen_row_idx = relevant_rows_copy.index[0]  # Choose the first index\n",
    "        elif n_valid > 1:\n",
    "            chosen_row_idx = relevant_rows_copy['difference_days'].idxmin()  # Choose the lowest difference\n",
    "        elif n_valid == 1:\n",
    "            chosen_row_idx = relevant_rows_copy[valid_mask].index[0]  # Choose the valid candidate\n",
    "\n",
    "    if chosen_row_idx is None:\n",
    "        print(relevant_rows_copy)  # Print relevant rows if no choice is made\n",
    "\n",
    "    return chosen_row_idx, applied_rule  # Return chosen index and applied rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_ohdl(prepared_extract_data_ohdl, prepared_supp_data):\n",
    "    \"\"\"\n",
    "    Executes the linkage process for OHLD data by iterating through prepared extract data and linking with supplemental data.\n",
    "    \n",
    "    Parameters:\n",
    "    - prepared_extract_data_ohdl (pd.DataFrame): The extracted data prepared for linkage.\n",
    "    - prepared_supp_data (pd.DataFrame): The supplemental data containing relevant hearing details.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: The final linked DataFrame without duplicates and irrelevant link types.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Concatenate all linked rows by processing each row in prepared extract data\n",
    "    all_linked_rows = pd.concat([link_row(prepared_extract_data_ohdl.loc[i], prepared_supp_data) for i in range(len(prepared_extract_data_ohdl))]).reset_index(drop=True)\n",
    "\n",
    "    # Remove duplicates from linked rows based on 'letter_id'\n",
    "    linked_rows_no_duplicates = all_linked_rows.drop_duplicates(subset=('letter_id'), keep=False)\n",
    "    linked_rows_duplicates = all_linked_rows[all_linked_rows.duplicated('letter_id', keep=False)]  # Identify duplicated rows\n",
    "\n",
    "    chosen_row_idxs_list = []  # List to hold chosen indices\n",
    "    applied_rules_list = []  # List to hold applied rules\n",
    "\n",
    "    # Process each unique letter ID for duplicated rows\n",
    "    for letter_id in linked_rows_duplicates['letter_id'].unique():\n",
    "        linked_rows_duplicates_all = linked_rows_duplicates[linked_rows_duplicates['letter_id'] == letter_id]\n",
    "        chosen_row_idx, applied_rule = match_idx(linked_rows_duplicates_all)  # Find the best candidate\n",
    "        chosen_row_idxs_list.append(chosen_row_idx)  # Store chosen index\n",
    "        applied_rules_list.append(applied_rule)  # Store applied rule\n",
    "\n",
    "    # Create relevant DataFrame for duplicated rows with applied rules\n",
    "    linked_rows_duplicates_relevant = linked_rows_duplicates.loc[chosen_row_idxs_list].copy()\n",
    "    linked_rows_duplicates_relevant['applied_rule'] = applied_rules_list\n",
    "\n",
    "    # Combine linked rows without duplicates with relevant duplicated rows\n",
    "    linked_rows_relevant = pd.concat([linked_rows_no_duplicates, linked_rows_duplicates_relevant.drop('applied_rule', axis=1)]).reset_index(drop=True)\n",
    "\n",
    "    # Define link types to exclude from final data\n",
    "    link_types_exclude = ['closest day, non valid decision', 'same day, non valid decision', 'closest day, valid decision']\n",
    "\n",
    "    # Filter out excluded link types from the final data\n",
    "    linked_data = linked_rows_relevant.drop(linked_rows_relevant[linked_rows_relevant['link_type'].isin(link_types_exclude)].index).reset_index(drop=True)\n",
    "    # Drop unnecessary columns from final data\n",
    "    linked_data = linked_data.drop(['decision_valid', 'hearing_date_supp'], axis=1)\n",
    "    # Rename date column\n",
    "    linked_data.rename(columns={'hearing_date_letter': 'hearing_date'}, inplace=True)\n",
    "\n",
    "    return linked_data  # Return the final linked DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_linkage(prepared_extract_data, prepared_supp_data, letter_type):\n",
    "    \"\"\"\n",
    "    Links the prepared extract data with the supplementary data based on the letter type.\n",
    "\n",
    "    This function runs the appropriate linkage process depending on the given `letter_type`.\n",
    "\n",
    "    Args:\n",
    "    - prepared_extract_data (pd.DataFrame): The extracted data prepared for linkage.\n",
    "    - prepared_supp_data (pd.DataFrame): The supplementary data containing relevant hearing details.\n",
    "    - letter_type (str): Type of the letter, either 'mca' or 'oh'.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The final linked DataFrame without duplicates and irrelevant link types.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Determine linkage function based on letter type\n",
    "    if letter_type == 'mca':\n",
    "        # Link data for 'mca' letter type\n",
    "        linked_data = link_mcadl(prepared_extract_data, prepared_supp_data)\n",
    "    elif letter_type == 'oh':\n",
    "        # Link data for 'oh' letter type\n",
    "        linked_data = link_ohdl(prepared_extract_data, prepared_supp_data)\n",
    "    \n",
    "    return linked_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_linkage_process(folder_path_segment, simplified_data, supp_data, letter_type):\n",
    "    \"\"\"\n",
    "    Executes the entire linkage process, including extraction, preparation, and linking.\n",
    "\n",
    "    This function orchestrates the process by running extraction, preparing both the \n",
    "    supplementary data and extracted data, and finally linking the data together.\n",
    "\n",
    "    Args:\n",
    "    - folder_path_segment (str): The path to the folder containing the original decision letters.\n",
    "    - simplified_data (pd.DataFrame): Dataframe with simplified offending history details to join with extracted data.\n",
    "    - supp_data (pd.DataFrame): The input DataFrame containing supplementary hearing data.\n",
    "    - letter_type (str): Type of the letter, either 'mca' or 'oh'.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The final linked data after the full linkage process.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Run the extraction process\n",
    "    extract_data = run_extraction(folder_path_segment, simplified_data, letter_type)\n",
    "    \n",
    "    # Step 2: Prepare the supplementary data\n",
    "    prepared_supp = prepare_supp(supp_data)\n",
    "    \n",
    "    # Step 3: Prepare the extracted data based on the letter type\n",
    "    prepared_extract = prepare_extract(extract_data, prepared_supp, letter_type)\n",
    "    \n",
    "    # Step 4: Run the linkage process based on the letter type\n",
    "    linked_data = run_linkage(prepared_extract, prepared_supp, letter_type)\n",
    "    \n",
    "    return linked_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the simplified offending history data for each letter type\n",
    "simplified_mcadl = pd.read_excel('../data/primary_data/extract/mcadl/simplified_mcadl.xlsx', dtype={'letter_id': str})\n",
    "simplified_ohdl = pd.read_excel('../data/primary_data/extract/ohdl/simplified_ohdl.xlsx', dtype={'letter_id': str})\n",
    "# Load supp data\n",
    "supp_data = pd.read_excel('../data/supplementary_data/supp_data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p0/kd5c1wdj7_d6n50vl898smy00000gn/T/ipykernel_79823/312419578.py:55: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  relevant_data = pd.concat([relevant_data, relevant_row])\n",
      "/var/folders/p0/kd5c1wdj7_d6n50vl898smy00000gn/T/ipykernel_79823/312419578.py:55: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  relevant_data = pd.concat([relevant_data, relevant_row])\n",
      "/var/folders/p0/kd5c1wdj7_d6n50vl898smy00000gn/T/ipykernel_79823/886604316.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  supp_data_relevant_row['link_type'] = link_type  # Add link type to the final row\n"
     ]
    }
   ],
   "source": [
    "linked_mcadl = run_linkage_process('../data/primary_data/letters/mcadl/segmented_dls/', simplified_mcadl, supp_data, 'mca')\n",
    "linked_ohdl = run_linkage_process('../data/primary_data/letters/ohdl/segmented_dls/', simplified_ohdl, supp_data, 'oh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save linked data\n",
    "linked_mcadl.to_excel('../data/linked_data/mcadl/linked_mcadl.xlsx', index=False)\n",
    "linked_ohdl.to_excel('../data/linked_data/ohdl/linked_ohdl.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
