{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from docloader import DocLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(doc_trf, doc_ner):\n",
    "    \"\"\"\n",
    "    Extracts crime entities and letter contexts from a single letter using two NLP models:\n",
    "    1. nlp_option_trf for sentence parsing.\n",
    "    2. nlp_option_ner for crime-specific named entity recognition (NER).\n",
    "\n",
    "    Args:\n",
    "    - doc_trf (DocLoader): A trf spaCy document object.\n",
    "    - doc_ner (DocLoader): A ner spaCy document object.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame containing the identified crimes, and sentence context.\n",
    "    \"\"\"\n",
    "    final_rows = []  # Empty list for rows in the resulting pd.DataFrame to be appended to \n",
    "    sents_info = []  # Empty list for tuples of letter sentence information\n",
    "    \n",
    "    # Collect all sentences from the document\n",
    "    for sent in doc_trf.sents:\n",
    "        if sent.text != '-':  # Skip empty sentences or single dash\n",
    "            # Append the sentence start index, end index, and text as a tuple to the all_sents list\n",
    "            sents_info.append((sent.start_char, sent.end_char, sent.text))\n",
    "    \n",
    "    ents_info = set()  # Empty set to hold all entity info tuples\n",
    "\n",
    "    # Extract entities labeled as 'CRIM' from the NER-processed document\n",
    "    if doc_ner.ents:\n",
    "        for ent in doc_ner.ents:\n",
    "            if ent.label_ == 'CRIM':\n",
    "                crime = ent.text\n",
    "                crime_index = (ent.start_char, ent.end_char)\n",
    "                ents_info.add((crime_index, crime))\n",
    "\n",
    "        # Link crimes to their respective sentences\n",
    "        offence_lookup = {}\n",
    "        for i, sent_info in enumerate(sents_info):\n",
    "            for ent_info in ents_info:\n",
    "                # Check if crime entity falls within the sentence bounds\n",
    "                if sent_info[0] <= ent_info[0][0] and sent_info[1] >= ent_info[0][1]:\n",
    "                    if i in offence_lookup:\n",
    "                        offence_lookup[i].append(ent_info)\n",
    "                    else:\n",
    "                        offence_lookup[i] = [ent_info]\n",
    "\n",
    "        # Clean and format sentences for context extraction\n",
    "        sents_cleaned = []\n",
    "        for start, end, text in sents_info:\n",
    "            sents_newline = re.sub(r'\\n+', ' ', text).strip()  # Remove newlines and extra spaces\n",
    "            sents_cleaned.append((start, end, sents_newline))\n",
    "\n",
    "        # Extract the crime and its context (previous, current, and next sentence)\n",
    "        for i, sent in enumerate(sents_cleaned):\n",
    "            if i in offence_lookup:  # Sentence contains a crime\n",
    "                context = []\n",
    "\n",
    "                # Add previous sentence to context if it doesn't contain an offence\n",
    "                if i > 0 and (i-1) not in offence_lookup:\n",
    "                    context.append(sents_cleaned[i-1][2])\n",
    "\n",
    "                # Add the current sentence\n",
    "                context.append(sent[2])\n",
    "\n",
    "                # Add next sentence to context if it doesn't contain an offence\n",
    "                if i < len(sents_info) - 1 and (i+1) not in offence_lookup:\n",
    "                    context.append(sents_cleaned[i+1][2])\n",
    "\n",
    "                # Collect information for each crime in the sentence\n",
    "                for match in offence_lookup[i]:\n",
    "                    raw_index, raw_crime = match\n",
    "\n",
    "                    final_rows.append({\n",
    "                        'crime': raw_crime,  # Crime text\n",
    "                        'context': ' '.join(context),  # Join all context sentences\n",
    "                    })\n",
    "\n",
    "    else:\n",
    "        # If no crimes are found, append a row indicating \"no crime\" and \"no context\"\n",
    "        final_rows.append({\n",
    "            'crime': 'no crime',\n",
    "            'context': 'no context',\n",
    "        })\n",
    "    \n",
    "    # Convert the collected data to rows of a DataFrame and return\n",
    "    letter_extract = pd.DataFrame(final_rows)\n",
    "    return letter_extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_extraction(loader_trf, loader_ner):\n",
    "    \"\"\"\n",
    "    Runs the extraction process for all letters in a folder.\n",
    "\n",
    "    Args:\n",
    "    - loader_trf (DocLoader): A trf DocLoader instance to load documents.\n",
    "    - loader_ner (DocLoader): A custon ner DocLoader instance to load documents. \n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A combined DataFrame containing all letters' extracted crimes and contexts.\n",
    "    \"\"\"\n",
    "    # Find all .txt files in the raw folder\n",
    "    letter_paths = set(loader_ner.all_letter_paths_extract())\n",
    "    \n",
    "    # Create an empty DataFrame to hold results from all letters\n",
    "    letters_extract = pd.DataFrame(columns=['letter_id', 'crime', 'context'])\n",
    "\n",
    "    # Process each letter, run extraction, and concatenate the results\n",
    "    for letter_path in letter_paths:\n",
    "        doc_trf = loader_trf.load_extract(letter_path)\n",
    "        doc_ner = loader_ner.load_extract(letter_path)\n",
    "        letter_extract = extract(doc_trf, doc_ner)\n",
    "        letter_extract['letter_id'] = letter_path.stem\n",
    "        letters_extract = pd.concat([letters_extract, letter_extract])\n",
    "\n",
    "    letters_extract[letters_extract['crime'] == 'no crime']\n",
    "\n",
    "    return letters_extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ericamcgovern/opt/miniconda3/envs/phdpipe/lib/python3.9/site-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_pipeline' (0.0.0) was trained with spaCy v3.3.1 and may not be 100% compatible with the current version (3.7.4). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "# Load the NLP models\n",
    "nlp_trf = spacy.load('en_core_web_trf')\n",
    "nlp_ner = spacy.load('../data/models/ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths for letters and caches\n",
    "folder_path_segment_mcadl = '../data/primary_data/letters/mcadl/segmented_dls/'\n",
    "folder_path_segment_ohdl = '../data/primary_data/letters/ohdl/segmented_dls/'\n",
    "\n",
    "cache_path_trf_mcadl = '../data/primary_data/letters/mcadl/caches/en_core_web_trf_extract'\n",
    "cache_path_trf_ohdl = '../data/primary_data/letters/ohdl/caches/en_core_web_trf_extract'\n",
    "\n",
    "cache_path_ner_mcadl = '../data/primary_data/letters/mcadl/caches/ner_extract'\n",
    "cache_path_ner_ohdl= '../data/primary_data/letters/ohdl/caches/ner_extract'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the DocLoader instances\n",
    "loader_trf_mcadl = DocLoader(nlp_trf, folder_path_segment_mcadl, cache_path_trf_mcadl)\n",
    "loader_trf_ohdl = DocLoader(nlp_trf, folder_path_segment_ohdl, cache_path_trf_ohdl)\n",
    "\n",
    "loader_ner_mcadl = DocLoader(nlp_ner, folder_path_segment_mcadl, cache_path_ner_mcadl)\n",
    "loader_ner_ohdl = DocLoader(nlp_ner, folder_path_segment_ohdl, cache_path_ner_ohdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the crime extraction process for MCADL and OHDL letters\n",
    "extract_mcadl = run_extraction(loader_trf_mcadl, loader_ner_mcadl)\n",
    "extract_ohdl = run_extraction(loader_trf_ohdl, loader_ner_ohdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the extraction results \n",
    "extract_mcadl.to_excel('../data/primary_data/extract/mcadl/extract_mcadl.xlsx', index=False)\n",
    "extract_ohdl.to_excel('../data/primary_data/extract/ohdl/extract_ohdl.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
